# 2025-AdvancedRAG

## Hands-on 4: การตอบคำถามด้วย LLM ภายใต้บริบทของข้อมูลที่ค้นหาได้ (RAG)

## รายละเอียด
ต่อเนื่องจาก Hands-on 3 ที่ได้เรียนรู้การค้นหาข้อมูลโดยใช้ Retriever กับ OpenSearch แล้ว Hands-on 4 นี้จะเน้นการพัฒนาต่อยอดเป็นระบบ RAG (Retrieval-Augmented Generation) เต็มรูปแบบ โดยนำข้อมูลที่ค้นหาได้ (Retrieval) มาสร้างเป็น prompt เพื่อส่งให้ LLM (Qwen2.5) ตอบคำถาม (Generation) อย่างถูกต้องภายใต้บริบทของข้อมูลที่ค้นพบ

## จุดประสงค์การเรียนรู้
* เข้าใจหลักการทำงานของ RAG (Retrieval-Augmented Generation) แบบเต็มรูปแบบ
* เรียนรู้การสร้าง prompt ที่มีประสิทธิภาพสำหรับส่งให้ LLM ตอบคำถาม
* ฝึกการวิเคราะห์และประเมินคำตอบที่ได้จาก LLM
* เข้าใจวิธีการเชื่อมต่อกับ LLM API และการรับส่งข้อมูล
* พัฒนาระบบค้นหาและตอบคำถามสุขภาพแบบครบวงจร

## ขั้นตอนการทำงาน
1. ติดตั้ง LlamaIndex, dependencies และไลบรารีที่จำเป็น
2. ตั้งค่าเชื่อมต่อกับ OpenSearch และ LLM API (Ollama กับ Qwen2.5)
3. นำ search_api จาก Hands-on 3 มาพัฒนาต่อให้ทำงานกับ Hybrid Search
4. พัฒนาฟังก์ชันสร้าง prompt (create_llm_prompt) ที่มีประสิทธิภาพสำหรับส่งให้ LLM
5. สร้างฟังก์ชันเรียกใช้ LLM API (query_llm) เพื่อส่งข้อมูลและรับคำตอบกลับมา
6. พัฒนาฟังก์ชันการค้นหาและตอบคำถาม (search_and_answer) ที่ผสมผสานทั้งการค้นหาและการตอบคำถาม
7. ทดสอบระบบด้วยคำถามที่กำหนดไว้ล่วงหน้า
8. อนุญาตให้ผู้ใช้ป้อนคำถามเพิ่มเติม

## คำอธิบายโค้ด
- **การติดตั้ง**: ติดตั้งไลบรารีที่จำเป็น เช่น llama-index, llama-index-embeddings-huggingface, llama-index-vector-stores-opensearch, torch, requests, nest_asyncio
- **การเชื่อมต่อ OpenSearch**: กำหนดค่า endpoint, index name และฟิลด์ที่ใช้ในการค้นหา โดยเชื่อมต่อกับ Vector Index ที่บันทึกไว้แล้ว
- **การเชื่อมต่อ LLM API**: กำหนดค่า endpoint (Ollama API) และโมเดล (Qwen2.5:32b) ที่ใช้ในการตอบคำถาม
- **ฟังก์ชันค้นหา (search_api)**: พัฒนาต่อจาก Hands-on 3 ให้ค้นหาข้อมูลโดยใช้ Hybrid Search และคืนผลลัพธ์แบบไม่ตัดทอน
- **ฟังก์ชันแสดงผล (display_search_results)**: แสดงผลลัพธ์การค้นหาในรูปแบบที่อ่านง่าย
- **ฟังก์ชันสร้าง prompt (create_llm_prompt)**: สร้าง prompt ที่มีโครงสร้างเหมาะสมสำหรับ Qwen2.5 ประกอบด้วยคำสั่ง, บริบทจากผลการค้นหา, และคำถาม
- **ฟังก์ชันเรียกใช้ LLM (query_llm)**: ส่ง prompt ไปยัง Ollama API และรับคำตอบกลับมา พร้อมทำความสะอาดข้อความ
- **ฟังก์ชันค้นหาและตอบคำถาม (search_and_answer)**: รวมกระบวนการทั้งหมดตั้งแต่ค้นหาข้อมูล, แสดงผลการค้นหา, สร้าง prompt, เรียกใช้ LLM, และแสดงคำตอบ

## คำถามที่กำหนดไว้ล่วงหน้า
ระบบมีคำถามที่กำหนดไว้ล่วงหน้าเพื่อทดสอบการค้นหาและตอบคำถาม ได้แก่:
1. โรคหัดและโรคหัดเยอรมันแตกต่างกันอย่างไร? (คำถามเปรียบเทียบ)
2. อธิบายสาเหตุของโรคหัดเยอรมันและการป้องกัน (คำถามหลายประเด็น)
3. ทำไมโรคหัดเยอรมันจึงมีอันตรายกับหญิงตั้งครรภ์? (คำถามวิเคราะห์เชิงลึก)
4. ถ้าคนที่ฉีดวัคซีนป้องกันโรคหัดเยอรมันแล้ว จะมีโอกาสติดเชื้อหรือไม่? (คำถามสมมติเหตุการณ์)
5. โรคหัดเยอรมันมีผลกระทบอย่างไรต่อระบบสาธารณสุขและเศรษฐกิจของประเทศ? (คำถามข้ามสาขา)
6. การรักษาโรคหัดเยอรมันที่ดีที่สุดคืออะไร? (คำถามกำกวม)

## โครงสร้าง Prompt
Prompt ที่ส่งให้ LLM มีโครงสร้างดังนี้:
1. **คำสั่ง**: แนะนำ LLM ให้ตอบคำถามอย่างถูกต้องและมีประโยชน์ พร้อมทั้งใช้เทคนิค Chain-of-Thought, เหตุผลเชิงตรรกะ, การประเมินคุณภาพข้อมูล, การแสดงการคำนวณ, และการตรวจสอบความถูกต้อง
2. **บริบท**: แสดงข้อมูลที่ค้นพบจากการค้นหา โดยระบุแหล่งที่มาและเนื้อหาเต็ม
3. **คำถาม**: คำถามที่ผู้ใช้ต้องการคำตอบ
4. **การคิด**: ชี้แนะให้ LLM แสดงกระบวนการคิดก่อนตอบคำถาม

## การตอบคำถามของ LLM
LLM (Qwen2.5) จะตอบคำถามโดยอ้างอิงจากข้อมูลที่ค้นพบเท่านั้น ไม่ใช้ความรู้ภายในของตัวเอง ซึ่งช่วยให้คำตอบมีความถูกต้องและเชื่อถือได้มากขึ้น โดยมีลักษณะดังนี้:
- **มีการอ้างอิงข้อมูล**: อ้างอิงแหล่งที่มาของข้อมูลที่ใช้ในการตอบคำถาม
- **แสดงกระบวนการคิด**: ใช้เทคนิค Chain-of-Thought เพื่อแสดงขั้นตอนการคิดก่อนสรุปคำตอบ
- **ตอบตรงประเด็น**: ตอบคำถามอย่างตรงประเด็นและครอบคลุม
- **สรุปข้อมูล**: สรุปข้อมูลจากหลายแหล่งเพื่อให้คำตอบที่สมบูรณ์

## การใช้งาน

### ทางเลือกในการรันโค้ด

1. **Google Colab (แนะนำ)**
   * เข้าถึงโค้ดได้ที่: [https://github.com/aekanun2020/2025-AdvancedRAG/blob/main/SENT_v4_Hands_on_4_Answering_within_Context.ipynb](https://github.com/aekanun2020/2025-AdvancedRAG/blob/main/SENT_v4_Hands_on_4_Answering_within_Context.ipynb)
   * สามารถรันได้ทันทีโดยไม่ต้องติดตั้งอะไรเพิ่มเติม
   * แนะนำให้รันโค้ดแบบต่อเนื่องทีละเซลล์เพื่อดูผลลัพธ์ทุกขั้นตอน

2. **Local Notebook ด้วย Conda (แนะนำสำหรับการรันในเครื่อง)**
   * สำหรับทั้งผู้ใช้ **Windows** และ **Mac M-series**:
     - ติดตั้ง Miniconda หรือ Anaconda: [ดาวน์โหลด Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html)
     - สร้าง Conda environment ด้วย Python 3.8.18:
       ```bash
       conda create -n advrag python=3.8.18
       conda activate advrag
       ```
     - ติดตั้ง dependencies:
       ```bash
       pip install llama-index llama-index-embeddings-huggingface llama-index-vector-stores-opensearch requests nest_asyncio jupyter torch
       ```
     - ตรวจสอบว่าใช้ Python 3.8.18 จริง:
       ```bash
       python --version
       ```
     - ดาวน์โหลดไฟล์ notebook และรันด้วย:
       ```bash
       jupyter notebook
       ```
     
   * คำแนะนำเพิ่มเติมสำหรับผู้ใช้ **Mac M-series**:
     - Conda จะจัดการ compatibility สำหรับ Apple Silicon โดยอัตโนมัติ
     - หากยังมีปัญหากับ libraries บางตัว ให้ติดตั้งเวอร์ชันที่เฉพาะเจาะจงสำหรับ ARM64:
       ```bash
       conda install -c conda-forge package-name
       ```

3. **การใช้งานกับ Ollama**
   * ต้องมีการติดตั้ง Ollama และดาวน์โหลดโมเดล Qwen2.5:32b ก่อน:
     ```bash
     # ติดตั้ง Ollama ตามคำแนะนำที่ https://ollama.com/download
     # เริ่มต้น Ollama service
     ollama serve
     # ดาวน์โหลดโมเดล Qwen2.5:32b
     ollama pull qwen2.5:32b
     ```

เมื่อรันโค้ดแล้ว ระบบจะเชื่อมต่อกับ OpenSearch เพื่อค้นหาข้อมูลและเชื่อมต่อกับ Ollama API เพื่อใช้ Qwen2.5:32b ในการตอบคำถาม ระบบจะแสดงผลลัพธ์การค้นหาและคำตอบจาก LLM สำหรับคำถามที่กำหนดไว้ล่วงหน้า และอนุญาตให้ผู้ใช้ป้อนคำถามเพิ่มเติมได้

## ข้อควรระวัง
- **การเชื่อมต่อ API**: ต้องมีการเชื่อมต่ออินเทอร์เน็ตที่สามารถเข้าถึง OpenSearch endpoint และ Ollama API endpoint ได้
- **ทรัพยากร**: การใช้ Qwen2.5:32b ต้องใช้ทรัพยากรเครื่องค่อนข้างสูง โดยเฉพาะหน่วยความจำ (RAM) และ GPU
- **ความล่าช้า**: การสร้าง embedding และการตอบคำถามด้วย LLM อาจใช้เวลานาน โดยเฉพาะในครั้งแรกที่ต้องโหลดโมเดล
- **ขนาดข้อมูล**: prompt ที่ส่งให้ LLM อาจมีขนาดใหญ่ หากผลการค้นหามีหลายรายการ อาจต้องลดจำนวน top_k ลง

## ประโยชน์ของ RAG เมื่อเทียบกับการใช้ LLM ล้วนๆ
- **ความถูกต้อง**: LLM ตอบคำถามโดยอ้างอิงจากข้อมูลที่มีอยู่จริง ไม่ใช่ความรู้ภายในที่อาจไม่ถูกต้องหรือล้าสมัย
- **ความโปร่งใส**: สามารถตรวจสอบแหล่งที่มาของข้อมูลที่ใช้ในการตอบคำถามได้
- **การปรับปรุงข้อมูล**: สามารถปรับปรุงฐานข้อมูลได้โดยไม่ต้องเทรนโมเดล LLM ใหม่
- **ความเฉพาะเจาะจง**: สามารถตอบคำถามเฉพาะทางที่ LLM ทั่วไปอาจไม่มีข้อมูลเพียงพอ
- **ลดปัญหา Hallucination**: ช่วยลดปัญหา LLM แต่งข้อมูลที่ไม่มีอยู่จริง